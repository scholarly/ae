---
title: "Wikipedia Vandalism"
author: "Terrel Shumway"
date: "04/11/2015"
output: html_document
---

First, we download and read the file `wiki.csv`...

```{r,echo=FALSE}

remote = "https://courses.edx.org/c4x/MITx/15.071x_2/asset/wiki.csv"
local = "wiki.csv"
if(!file.exists(local)){
  download.file(remote,local,"curl")
}
wiki = read.csv(local,stringsAsFactors=F)
str(wiki)
```

## Problem 1.1
Problem 1.1: There are `r sum(wiki$Vandal)` vandalism cases in the dataset.


## Problem 1.2: Bag of Words

```{r}
library(tm)
library(SnowballC)
corpus = Corpus(VectorSource(wiki$Added))
corpus = tm_map(corpus,PlainTextDocument)
corpus = tm_map(corpus,removeWords, stopwords("english"))
corpus = tm_map(corpus,stemDocument)

dtmA = DocumentTermMatrix(corpus)

```

Problem 1.2: There are `r ncol(dtmA)` terms in the document term matrix

```{r}
sparse = removeSparseTerms(dtmA,0.997)
```

Problem 1.3: There are `r ncol(sparse)` terms in the pruned matrix

```{r}
wordsAdded = as.data.frame(as.matrix(sparse))
colnames(wordsAdded) = paste("A",colnames(wordsAdded))
```

## Words removed

```{r}
corpus = Corpus(VectorSource(wiki$Removed))
corpus = tm_map(corpus,PlainTextDocument)
corpus = tm_map(corpus,removeWords, stopwords("english"))
corpus = tm_map(corpus,stemDocument)

dtmR = DocumentTermMatrix(corpus)

sparse = removeSparseTerms(dtmR,0.997)

wordsRemoved = as.data.frame(as.matrix(sparse))
colnames(wordsRemoved) = paste("R",colnames(wordsRemoved))

```

Problem 1.4: There are `r ncol(sparse)` terms in the pruned matrix

## Combine the bags

```{r}
wikiWords = cbind(wordsAdded,wordsRemoved)
wikiWords$Vandal = wiki$Vandal
```

## Split into train and test sets

```{r}
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal,SplitRatio=0.7)
train = subset(wikiWords,split)
test = subset(wikiWords,!split)
```

```{r}
N = nrow(test)
bl.c = table(test$Vandal)
bl.acc = bl.c[1]/N
```

Problem 1.5: The baseline model has accuracy `r bl.acc`.

## Build a CART model

```{r}

library(rpart)
library(rpart.plot)
m1 = rpart(Vandal~.,data=train,method="class")

m1.p = predict(m1,newdata=test,type="class")
m1.c = table(test$Vandal,m1.p)
m1.acc = (m1.c[1,1]+m1.c[2,2])/nrow(test)

```

Problem 1.6: The default CART model has accuracy `r m1.acc`.

```{r,echo=F}
prp(m1)
m1.splits = m1$cptable[nrow(m1$cptable),'nsplit']
```
Problem 1.7: `r m1.splits` terms

# Using Problem-specific knowledge

```{r}
ww2 = wikiWords
ww2$http = ifelse(grepl("http",wiki$Added,fixed=TRUE),1,0)
clinks = sum(ww2$http)
```
Problem 2.1: `r clinks` documents include `http`.

```{r}
train2 = subset(ww2,split)
test2 = subset(ww2,!split)

m2 = rpart(Vandal~.,data=train2,method="class")

m2.p = predict(m2,newdata=test2,type="class")
m2.c = table(test$Vandal,m2.p)
m2.acc = (m2.c[1,1]+m2.c[2,2])/nrow(test2)

```


Problem 2.2: The new CART model has accuracy `r m2.acc`.

```{r}

ww2$NumWordsAdded = rowSums(as.matrix(dtmA))
ww2$NumWordsRemoved = rowSums(as.matrix(dtmR))
awa = mean(ww2$NumWordsAdded)
```

Problem 2.3: Average of `r awa` words added.

```{r}
train3 = subset(ww2,split)
test3 = subset(ww2,!split)

m3 = rpart(Vandal~.,data=train3,method="class")

m3.p = predict(m3,newdata=test3,type="class")
m3.c = table(test$Vandal,m3.p)
m3.acc = (m3.c[1,1]+m3.c[2,2])/nrow(test3)

```


Problem 2.4: The new CART model has accuracy `r m3.acc`.

# Use Non-Textual Data

```{r}
ww3 = ww2
ww3$Minor = wiki$Minor
ww3$Loggedin = wiki$Loggedin


train4 = subset(ww3,split)
test4 = subset(ww3,!split)

m4 = rpart(Vandal~.,data=train4,method="class")

m4.p = predict(m4,newdata=test4,type="class")
m4.c = table(test$Vandal,m4.p)
m4.acc = (m4.c[1,1]+m4.c[2,2])/nrow(test4)

```
Problem 3.1: The new CART model has accuracy `r m4.acc`.
```{r,echo=FALSE}
prp(m4)
```

